import requests
from bs4 import BeautifulSoup
from typing import List, Optional
from urllib.parse import urljoin, quote
import re
from app.schemas.wiki import WikiPage, WikiSearchResult


class BOBWikiCrawler:
    def __init__(self):
        self.base_url = "https://kitribob.wiki"
        self.search_url = "https://kitribob.wiki/wiki/14ê¸°_êµìœ¡ìƒ"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
    
    def search_student(self, student_name: str) -> WikiSearchResult:
        """14ê¸° êµìœ¡ìƒ í˜ì´ì§€ì—ì„œ íŠ¹ì • í•™ìƒ ê²€ìƒ‰"""
        try:
            print(f"ğŸ” '{student_name}' ê²€ìƒ‰ ì‹œì‘...")
            
            # 14ê¸° êµìœ¡ìƒ ë©”ì¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            response = self.session.get(self.search_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # í•™ìƒ í˜ì´ì§€ ë§í¬ ì°¾ê¸°
            student_links = self._find_student_links(soup, student_name)
            
            pages = []
            for link in student_links:
                page = self._crawl_page(link)
                if page:
                    pages.append(page)
            
            result = WikiSearchResult(
                search_term=student_name,
                pages=pages,
                total_pages=len(pages)
            )
            
            print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(pages)}ê°œ í˜ì´ì§€ ë°œê²¬")
            return result
            
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return WikiSearchResult(
                search_term=student_name,
                pages=[],
                total_pages=0
            )
    
    def _find_student_links(self, soup: BeautifulSoup, student_name: str) -> List[str]:
        """í•™ìƒ ì´ë¦„ê³¼ ê´€ë ¨ëœ ë§í¬ë“¤ ì°¾ê¸°"""
        links = []
        
        # 1. ì§ì ‘ì ì¸ ì´ë¦„ ë§¤ì¹­ìœ¼ë¡œ ë§í¬ ì°¾ê¸°
        for link in soup.find_all('a', href=True):
            link_text = link.get_text(strip=True)
            href = link.get('href')
            
            # í•™ìƒ ì´ë¦„ì´ í¬í•¨ëœ ë§í¬ ì°¾ê¸°
            if student_name in link_text or student_name.replace(' ', '') in link_text.replace(' ', ''):
                full_url = urljoin(self.base_url, href)
                if full_url not in links:
                    links.append(full_url)
                    print(f"ğŸ“„ ë°œê²¬ëœ ë§í¬: {link_text} -> {full_url}")
        
        # 2. í…Œì´ë¸”ì—ì„œ í•™ìƒ ì •ë³´ ì°¾ê¸°
        for table in soup.find_all('table'):
            for row in table.find_all('tr'):
                cells = row.find_all(['td', 'th'])
                for cell in cells:
                    if student_name in cell.get_text(strip=True):
                        # ê°™ì€ í–‰ì—ì„œ ë§í¬ ì°¾ê¸°
                        for link in row.find_all('a', href=True):
                            full_url = urljoin(self.base_url, link.get('href'))
                            if full_url not in links:
                                links.append(full_url)
                                print(f"ğŸ“Š í…Œì´ë¸”ì—ì„œ ë°œê²¬: {link.get_text(strip=True)} -> {full_url}")
        
        return links
    
    def _crawl_page(self, url: str) -> Optional[WikiPage]:
        """ê°œë³„ í˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            print(f"ğŸ“– í˜ì´ì§€ í¬ë¡¤ë§: {url}")
            
            response = self.session.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # í˜ì´ì§€ ì œëª© ì¶”ì¶œ
            title = self._extract_title(soup)
            
            # í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ
            content = self._extract_content(soup)
            
            if not content.strip():
                print(f"âš ï¸ ë¹ˆ ë‚´ìš©: {url}")
                return None
            
            page = WikiPage(
                title=title,
                url=url,
                content=content[:5000],  # ë‚´ìš© ê¸¸ì´ ì œí•œ
                author=self._extract_author(soup)
            )
            
            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {title} ({len(content)} ë¬¸ì)")
            return page
            
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨ ({url}): {e}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """í˜ì´ì§€ ì œëª© ì¶”ì¶œ"""
        # h1 íƒœê·¸ì—ì„œ ì œëª© ì¶”ì¶œ
        h1 = soup.find('h1')
        if h1:
            return h1.get_text(strip=True)
        
        # title íƒœê·¸ì—ì„œ ì œëª© ì¶”ì¶œ
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.get_text(strip=True)
        
        return "ì œëª© ì—†ìŒ"
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ"""
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ ì œê±°
        for element in soup(['script', 'style', 'nav', 'header', 'footer']):
            element.decompose()
        
        # ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ ì°¾ê¸°
        content_selectors = [
            '#content',
            '.content',
            '#main',
            '.main',
            'article',
            '.wiki-content'
        ]
        
        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                text = content_div.get_text(separator='\n', strip=True)
                if text.strip():
                    return self._clean_text(text)
        
        # ì „ì²´ bodyì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (fallback)
        body = soup.find('body')
        if body:
            text = body.get_text(separator='\n', strip=True)
            return self._clean_text(text)
        
        return ""
    
    def _extract_author(self, soup: BeautifulSoup) -> Optional[str]:
        """ì‘ì„±ì ì •ë³´ ì¶”ì¶œ"""
        # ì¼ë°˜ì ì¸ ì‘ì„±ì í‘œì‹œ íŒ¨í„´ë“¤
        author_patterns = [
            '.author',
            '.creator',
            '[class*="author"]',
            '[class*="creator"]'
        ]
        
        for pattern in author_patterns:
            author_elem = soup.select_one(pattern)
            if author_elem:
                return author_elem.get_text(strip=True)
        
        return None
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ì—°ì†ëœ ê³µë°±/ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r'\n+', '\n', text)
        text = re.sub(r' +', ' ', text)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text

# ê¸€ë¡œë²Œ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
wiki_crawler = BOBWikiCrawler()
